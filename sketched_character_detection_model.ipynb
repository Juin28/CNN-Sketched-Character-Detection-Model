{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9788mVl2GJPl"
      },
      "source": [
        "# CNN Sketched Character Detection Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjZTKCOO4YDg"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif\" alt=\"teaser\" width=\"600\"/>\n",
        "\n",
        "\n",
        "Image credits: https://github.com/facebookresearch/AnimatedDrawings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS2YPG2a0_uE"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "citZm92Y4gvg"
      },
      "source": [
        "\n",
        "We are using the *Amateur Drawings Dataset*. The original dataset comprises over 178K (~50Gb) images and associated annotations of amateur drawings. Due to the limit of Google Colab, the assiduous TAs have prepared a smaller sample.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/dataset.png?raw=true\" alt=\"dataset_overview\" width=\"600\"/>\n",
        "\n",
        "The filtered dataset has 2K (~500Mb) amateur drawing images. And the JSON file records the annotation list. Each entry has the `src` field about the relative path to the image, and the `bbox` field is defined by `[x, y, width, height]`, where `x` and `y` is the horizontal and vertical position of the top left corner.\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"src\": \"images/1.png\",\n",
        "    \"bbox\": [\n",
        "        32.0,\n",
        "        56.888888888888886,\n",
        "        303.36507936507934,\n",
        "        428.69841269841265\n",
        "    ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA9ap3H9eAMX",
        "outputId": "15a4c7f3-ee7a-41af-f8f3-b9a9a30eaad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings and logs\n",
        "import warnings\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check whether a GPU environment is enabled\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if len(device_name) > 0 and __name__ == '__main__':\n",
        "    print(\"Found GPU at: {}\".format(device_name))\n",
        "elif __name__ == '__main__':\n",
        "    device_name = \"/device:CPU:0\"\n",
        "    print(\"No GPU, using {}.\".format(device_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJSolaNmFv7"
      },
      "source": [
        "# Data Loading \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAoKQZ7ksSKz"
      },
      "source": [
        "Here, we are going to download the dataset, transform the data format, and split it into the training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Td51TWAvu6i"
      },
      "outputs": [],
      "source": [
        "# Download the dataset from Amazon S3; It can take 1 min.\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/gt_bbox.json\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/images.zip\"\n",
        "\n",
        "!unzip -q images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZmKkff52eUm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_dataset(dataset, image_size=(224, 224)):\n",
        "  \"\"\"Preprocess the raw dataset\n",
        "  dataset: the parsed JSON file as the dataset\n",
        "  image_size: targeted image size for the model input\n",
        "\n",
        "  return an array of the image paths, the transformed images and ground truth bboxes\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  bboxes = []\n",
        "  urls = []\n",
        "\n",
        "  for item in dataset:\n",
        "    url = item['src']\n",
        "    urls.append(url)\n",
        "    original_image = load_img(item['src'], target_size=None)\n",
        "    original_width = original_image.width\n",
        "    original_height = original_image.height\n",
        "    image = original_image.resize(image_size)\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        "    images.append(image)\n",
        "\n",
        "    bbox = item['bbox']\n",
        "    x, y, width, height = bbox\n",
        "\n",
        "    new_x = x / original_width\n",
        "    new_y = y / original_height\n",
        "    new_width = width / original_width\n",
        "    new_height = height / original_height\n",
        "\n",
        "    new_bbox = [new_x, new_y, new_width, new_height]\n",
        "\n",
        "    bboxes.append(new_bbox)\n",
        "\n",
        "  urls = np.array(urls, dtype=str)\n",
        "  images = np.array(images, dtype=np.float32)\n",
        "  bboxes = np.array(bboxes, dtype=np.float32)\n",
        "  return urls, images, bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQr_XQpZdesk"
      },
      "outputs": [],
      "source": [
        "def split_dataset(urls, x, gt, test_split_ratio=0.2):\n",
        "    \"\"\"Split the dataset according to the test split ratio\n",
        "    urls: the input image paths\n",
        "    x: the input image data (np.ndarray) to be fed into model\n",
        "    gt: the ground truth boundinng box (np.ndarray)\n",
        "    test_split_ratio: the percentage of test dataset size in respect to the full dataset\n",
        "\n",
        "    return the train_url, train_x, train_y, and test_url, test_x, and test_y\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "    test_size = int( n * test_split_ratio)\n",
        "\n",
        "    test_x = x[:test_size]\n",
        "    test_y = gt[:test_size]\n",
        "    test_url = urls[:test_size]\n",
        "    train_x = x[test_size:]\n",
        "    train_y = gt[test_size:]\n",
        "    train_url = urls[test_size:]\n",
        "    return train_url, train_x, train_y, test_url, test_x, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNIKSM-ZOxiu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  N = 1000  # we only load N images for the assignment (max=2,000)\n",
        "  IMAGE_SIZE = (224, 224) # ResNet standard image size\n",
        "  DATASET_FILE = './gt_bbox.json'\n",
        "  SPLIT_TEST_RATIO = 0.2\n",
        "\n",
        "  with open(DATASET_FILE) as f:\n",
        "      dataset = json.load(f)[:N]\n",
        "  urls, images, bboxes = preprocess_dataset(dataset, image_size=IMAGE_SIZE)\n",
        "  train_url, train_x, train_y, test_url, test_x, test_y = split_dataset(urls, images, bboxes, SPLIT_TEST_RATIO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2DGwblQZHB8"
      },
      "source": [
        "# Model Compilation & Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pVnm5bnkLET"
      },
      "source": [
        "When using ResNet50 for classification, the model expects a fixed-size input image and outputs a 1D array where each entry corresponds to the probability that the image belongs to a particular class. However, for our task, which is bounding box prediction, the standard ResNet50 architecture needs to be adapted. Our goal is not to classify an entire image into a single category, but rather to predict the coordinates of a box that encloses an object within the image. These coordinates are usually represented by four values: (x, y, width, height).\n",
        "\n",
        "Therefore, we will remove its original output layer and add custom layers for our task. Check the [Functional API](https://www.tensorflow.org/guide/keras/functional_api) in Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_wNY_9tNMO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, Dropout\n",
        "\n",
        "def create_model(input_shape=(224, 224, 3)):\n",
        "  \"\"\"Create a CNN model for predicting the bounding box for drawings in an image\n",
        "  input_shape: the targeted image size\n",
        "\n",
        "  return the model architecture\n",
        "  \"\"\"\n",
        "  # load the pre-trained ResNet50 model without the top classification layer\n",
        "  base_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=False)\n",
        "  # freeze the base model layers\n",
        "  base_model.trainable = False\n",
        "  # add custom layers on top for bounding box prediction\n",
        "  model_x = base_model.output\n",
        "\n",
        "  model_x = Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, strides = 3)(model_x)\n",
        "\n",
        "  model_x = GlobalAveragePooling2D()(model_x)  # use global average pooling to flatten the output\n",
        "  model_x = Dropout(0.5)(model_x) # randomly drop out weights to avoid overfitting\n",
        "  model_x = Dense(64, activation='relu', kernel_initializer='random_normal', name=\"check_layer_1\")(model_x)  # add a fully connected layer\n",
        "  model_x = Dense(32, activation='relu', kernel_initializer='random_normal', name=\"check_layer_2\")(model_x)  # add a fully connected layer\n",
        "\n",
        "  model_x = Dense(4, activation='sigmoid', kernel_initializer=\"glorot_normal\", name=\"output_layer\")(model_x)\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=model_x)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJzKD8TJHXk5"
      },
      "outputs": [],
      "source": [
        "def se_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt):\n",
        "  \"\"\"Compute the square error (SE) based on the tensor of (x, y, w, h) for the predicted bbox and ground truth\n",
        "\n",
        "  return an (N, 1) tensor as the individual loss value\n",
        "  \"\"\"\n",
        "\n",
        "  tl_x = tf.square(x1_pred - x1_gt)\n",
        "  tl_y = tf.square(y1_pred - y1_gt)\n",
        "  tl = tl_x + tl_y\n",
        "\n",
        "  br_x = tf.square((x1_pred + w_pred) - (x1_gt + w_gt))\n",
        "  br_y = tf.square((y1_pred + h_pred) - (y1_gt + h_gt))\n",
        "  br = br_x + br_y\n",
        "\n",
        "  return tl + br"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B1iXuDLXr11"
      },
      "outputs": [],
      "source": [
        "def giou_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt):\n",
        "  \"\"\"Compute the giou loss based on the tensor of (x, y, w, h) for the predicted bbox and ground truth\n",
        "\n",
        "  return an (N, 1) tensor as the loss value\n",
        "  \"\"\"\n",
        "  c = 1e-7\n",
        "\n",
        "  I_x_diff = tf.maximum(( tf.minimum(x1_pred + w_pred, x1_gt + w_gt) - tf.maximum(x1_pred, x1_gt) ), 0)\n",
        "  I_y_diff = tf.maximum(( tf.minimum(y1_pred + h_pred, y1_gt + h_gt) - tf.maximum(y1_pred, y1_gt) ), 0)\n",
        "  area_I = tf.multiply(I_x_diff, I_y_diff)\n",
        "\n",
        "  area_U = tf.multiply(w_pred, h_pred) + tf.multiply(w_gt, h_gt) - area_I\n",
        "\n",
        "  IoU = area_I / (area_U + c)\n",
        "\n",
        "  C_x_diff = tf.maximum(x1_pred + w_pred, x1_gt + w_gt) - tf.minimum(x1_pred, x1_gt)\n",
        "  C_y_diff = tf.maximum(y1_pred + h_pred, y1_gt + h_gt) - tf.minimum(y1_pred, y1_gt)\n",
        "  area_C = tf.multiply(C_x_diff, C_y_diff)\n",
        "\n",
        "  GIoU = IoU - (area_C - area_U) / (area_C + c)\n",
        "\n",
        "  GIoU_loss = 1 - GIoU\n",
        "  return GIoU_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SJoXryfHDP1"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(name=\"loss\")\n",
        "def loss_func(pred, gt):\n",
        "  \"\"\"The loss function for model training.\n",
        "  pred: an (N, 4) numpy array of predicted value for x, y, w, h\n",
        "  gt: an (N, 4) numpy array of the ground truth value x, y, w, h\n",
        "\n",
        "  return a scalar value of the mean batch loss\n",
        "  \"\"\"\n",
        "  gt = tf.convert_to_tensor(gt, dtype=tf.float32)\n",
        "  pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
        "\n",
        "  x1_gt, y1_gt, w_gt, h_gt = tf.split(gt, 4, axis=-1)\n",
        "  x1_pred, y1_pred, w_pred, h_pred = tf.split(pred, 4, axis=-1)\n",
        "\n",
        "  # you can also try using the mse error\n",
        "  # loss = se_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt)\n",
        "  loss = giou_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt)\n",
        "  return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR_7Ei1EW2v0"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam', loss=loss_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCLU_TvGcj4j"
      },
      "source": [
        "You may run the following code to enable an advanced callback of TensorBoard. It is a visualization suite for models. You may inspect the real-time training logs through [scalar panel](https://www.tensorflow.org/tensorboard/scalars_and_keras) or gain a better understanding of the model architecture through the [model graph panel](https://www.tensorflow.org/tensorboard/graphs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaqR0Fem5NpN"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "if __name__ == '__main__':\n",
        "  # set up the tensorboard callback for reviewing real-time progress\n",
        "  log_dir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajtuJK2oKHsL"
      },
      "outputs": [],
      "source": [
        "# load the tensorboard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyI5lrXoMw9K"
      },
      "outputs": [],
      "source": [
        "# remember to click the refresh button to load updated logs while training.\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIljSChMIiWn"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  # train the model, you may reduce the epoch number to test the functionability at first\n",
        "  # as ResNet is a relatively large model, for 20 epochs, you may wait for 2 min using GPU, or 30 min using CPU\n",
        "  N_EPOCH = 20\n",
        "  model.fit(train_x, train_y, epochs=N_EPOCH, batch_size=64, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CByhPX_zj9Y9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_param_from_layer(model, layer_name):\n",
        "    layer = model.get_layer(name=layer_name)\n",
        "    weights = layer.get_weights()\n",
        "    # Concatenate the weights into a single numpy array\n",
        "    params = np.concatenate([w.flatten() for w in weights])\n",
        "    return params\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  param_1 = get_param_from_layer(model, 'check_layer_1').reshape((1, -1))\n",
        "  param_2 = get_param_from_layer(model, 'check_layer_2').reshape((1, -1))\n",
        "  proof = np.concatenate([param_1.T, param_2.T], axis=0)\n",
        "  with open('proof.pkl', 'wb') as f:\n",
        "    pickle.dump(proof, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA8ALhuapzSD"
      },
      "outputs": [],
      "source": [
        "# You can run this to save the model to your local machine to avoid re-training\n",
        "if __name__ == '__main__':\n",
        "  filename = 'model.keras'\n",
        "  model.save(filename)\n",
        "  model = tf.keras.models.load_model(filename, safe_mode=False, custom_objects={'loss_func': loss_func})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf5G4MnaZrEL"
      },
      "source": [
        "# Performance Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEw_LUe_0NF"
      },
      "source": [
        "We will examine the model performance based on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH6XooDo_p2w"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  results = model.evaluate(test_x, test_y)\n",
        "  print(f'The loss in the model: {results}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4YzXtk2tMjd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def visualize_bounding_box(image_path, predicted_bbox=None, groundtruth_bbox=None):\n",
        "  \"\"\"Plot the original image, the predicted bounding box, and groundtruth (if any)\n",
        "  image_path: the path to the image\n",
        "  predicted_bbox: the predicted bounding box for drawings in the image\n",
        "  groundtruth_bbox: the ground truth bounding box\n",
        "\n",
        "  return void\n",
        "  \"\"\"\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  fig, ax = plt.subplots(1, figsize=(5, 5))\n",
        "  ax.imshow(image)\n",
        "\n",
        "  def draw_boxes(bbox, color, label):\n",
        "    x, y, w, h = bbox\n",
        "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x, y-18, f'{label}', fontsize=12, verticalalignment='top', color=color)\n",
        "\n",
        "  if predicted_bbox is not None:\n",
        "      draw_boxes(predicted_bbox, 'red', 'Predicted')\n",
        "  if groundtruth_bbox is not None:\n",
        "      draw_boxes(groundtruth_bbox, 'green', 'Ground Truth')\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDn1jIuAkuNl"
      },
      "outputs": [],
      "source": [
        "def extract_failure_case(loss_function, pred, gt, k=5):\n",
        "  \"\"\" Extract failure cases with the worst performance.\n",
        "  model: the trained model\n",
        "  loss_function: a loss function that calculates the scalar loss value for pairwise prediction (4,) and results (4,)\n",
        "  pred: the predicted bounding box (scaled to the image size)\n",
        "  gt: the ground truth bbox in the test dataset\n",
        "  k: the number of top failures with the largest loss value\n",
        "\n",
        "  return the indexes to the top k failure case\n",
        "  \"\"\"\n",
        "  loss_values = [loss_function(pred[i], gt[i]) for i in range(pred.shape[0])]\n",
        "  indexes_top_failure = np.argsort(loss_values)[len(loss_values) - 1:len(loss_values) - 1 - k: -1]\n",
        "  \n",
        "  return indexes_top_failure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale_bbox(src, raw_pred, scaled_gt):\n",
        "  original_image = load_img(src, target_size=None)\n",
        "  original_width = original_image.width\n",
        "  original_height = original_image.height\n",
        "  def _rescale_bbox(bbox):\n",
        "    x, y, w, h = bbox\n",
        "    x = x * original_width\n",
        "    y = y * original_height\n",
        "    w = w * original_width\n",
        "    h = h * original_height\n",
        "    return np.array([x, y, w, h])\n",
        "  return _rescale_bbox(raw_pred), _rescale_bbox(scaled_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd9TmRAjLo3N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "# You may refer to https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  test_pred = model.predict(test_x)\n",
        "  n_failures = 20\n",
        "  worst_case_indexes = extract_failure_case(mean_absolute_error, test_pred, test_y, n_failures)\n",
        "  for k in range(n_failures):\n",
        "    idx = worst_case_indexes[k]\n",
        "    test_pred_instance, gt_instance = rescale_bbox(test_url[idx], test_pred[idx], test_y[idx])\n",
        "    visualize_bounding_box(test_url[idx], test_pred_instance, gt_instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-6GVY1wMGom"
      },
      "outputs": [],
      "source": [
        "def predict_bounding_box(model, src):\n",
        "  \"\"\"leverage the model to identify drawings in an image\n",
        "  model: a trained object detector model\n",
        "  src: the source of the image file\n",
        "\n",
        "  return [x, y, w , h] for the predicted bounding box in the original image\n",
        "  \"\"\"\n",
        "  \n",
        "  original_image = load_img(src, target_size=None)\n",
        "  original_width = original_image.width\n",
        "  original_height = original_image.height\n",
        "  image = original_image.resize((224, 224))\n",
        "  image = img_to_array(image)\n",
        "\n",
        "  input_prediction = preprocess_input(image)\n",
        "  input_prediction = np.expand_dims(input_prediction, axis=0)\n",
        "  prediction = model.predict(input_prediction)\n",
        "\n",
        "  x, y, w, h = prediction[0]\n",
        "\n",
        "  ori_x = x * original_width\n",
        "  ori_y = y * original_height\n",
        "  ori_w = w * original_width\n",
        "  ori_h = h * original_height\n",
        "\n",
        "  return [ori_x, ori_y, ori_w, ori_h]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVwMUveqcBw_"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "def inspect(model, dataset, k=None):\n",
        "  \"\"\"Visually inspect one instance with its predicted and ground truth bbox\n",
        "  model: the trained model\n",
        "  dataset: the full dataset with image source and ground truth\n",
        "  k: the index of the image under inspection\n",
        "\n",
        "  return void\n",
        "  \"\"\"\n",
        "  if k is None:\n",
        "    k = randint(0, len(dataset)-1)\n",
        "  src = dataset[k]['src']\n",
        "  ground_truth = dataset[k]['bbox']\n",
        "  pred = predict_bounding_box(model, src)\n",
        "  visualize_bounding_box(src, pred, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24qDYydVSeNm"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  inspect(model, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPx4aSU4nUjt"
      },
      "outputs": [],
      "source": [
        "# YOU MUST COMMENT OUT THIS CELL WHEN HANDING IN THE CODE\n",
        "\n",
        "# Let us take a look at how your model performs on the drawings of Desmond's kids XD!\n",
        "!wget -O earnest.jpg \"https://home.cse.ust.hk/~desmond/images/ernest/peacock.jpg\" > /dev/null 2>&1\n",
        "!wget -O elvis.jpg \"https://home.cse.ust.hk/~desmond/images/elvis/elvis-pokemon.jpg\" > /dev/null 2>&1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  earnest_pred = predict_bounding_box(model, \"earnest.jpg\")\n",
        "  visualize_bounding_box(\"earnest.jpg\", earnest_pred, None)\n",
        "  elvis_pred = predict_bounding_box(model, \"elvis.jpg\")\n",
        "  visualize_bounding_box(\"elvis.jpg\", elvis_pred, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdeSEahcmtmo"
      },
      "outputs": [],
      "source": [
        "# YOU MUST COMMENT OUT THIS CELL WHEN HANDING IN THE CODE\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/grading_paths.pkl\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/grading_images.zip\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/testing_paths.pkl\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/testing_images.zip\"\n",
        "\n",
        "!unzip -q grading_images.zip\n",
        "!unzip -q testing_images.zip"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
